{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel.talasso/FT_LLM_FL/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append(\".\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.template import TEMPLATE_DICT\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "template = TEMPLATE_DICT['alpaca'][0]\n",
    "MODEL_NAME = 'TinyLlama/TinyLlama_v1.1'\n",
    "DATASET_NAME = \"CohereForAI/aya_dataset\"\n",
    "DEVICE = 'cuda:0'\n",
    "EVALSET_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_model(path, round, cluster = -1):\n",
    "\n",
    "    if cluster == -1:\n",
    "        path = path + f'/checkpoint-{round}'\n",
    "    else:\n",
    "        path = path + f'/cluster_{cluster}_checkpoint-{round}'\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16,\n",
    "                                                quantization_config = BitsAndBytesConfig(\n",
    "                                                                        load_in_4bit=True,\n",
    "                                                                        bnb_4bit_use_double_quant=True,\n",
    "                                                                        bnb_4bit_quant_type=\"nf4\",\n",
    "                                                                        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                                                    ),\n",
    "                                                device_map={\"\": Accelerator().local_process_index})\n",
    "    \n",
    "    model = PeftModel.from_pretrained(model, path).to(DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, device=DEVICE)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_data(DATASET_NAME, EVALSET_LEN, languages):\n",
    "    \n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\", )\n",
    "    dataset = dataset.filter(lambda x: x['language'] in ['English', 'Swedish', 'German', 'Portuguese', 'Spanish'])\n",
    "    dataset_splited = dataset.train_test_split(test_size= 0.2, seed=0)\n",
    "    dataset_test = dataset_splited['test']\n",
    "    dataset = dataset_test.filter(lambda x: x['language'] in languages)\n",
    "    dataset_len = min(len(dataset), EVALSET_LEN)\n",
    "    dataset = dataset.select(range(dataset_len))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_loss_in_dataset(dataset, model, tokenizer, batch_size=8):\n",
    "    #create a new feature in dataset that represents the full text (input + targets)\n",
    "    dataset = dataset.map(lambda x: {'text': x['inputs'] + x['targets']})\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    loss = 0\n",
    "    for data in tqdm(dataloader):\n",
    "        input_ids = tokenizer(data['text'], return_tensors='pt').input_ids.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, labels=input_ids)\n",
    "            loss += output.loss.item()\n",
    "    return loss/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(instruction, output, model, tokenizer, device = DEVICE):\n",
    "    # Combine instruction and output\n",
    "    combined = f\"{instruction} {output}\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(combined, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "    return loss.item() #torch.exp(loss).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexit_in_dataset(dataset, model, tokenizer, device = DEVICE):\n",
    "    model.eval().to(device)\n",
    "    perplexities = []\n",
    "\n",
    "    for sample in tqdm(dataset):\n",
    "        instruction = sample['inputs']\n",
    "        output = sample['targets']\n",
    "\n",
    "        perplexity = calculate_perplexity(instruction, output, model, tokenizer, device)\n",
    "        perplexities.append(perplexity)\n",
    "\n",
    "    # 5. Calculate mean perplexity\n",
    "    mean_perplexity = np.mean(perplexities)\n",
    "    std_perplexity = np.std(perplexities)\n",
    "\n",
    "    return mean_perplexity, std_perplexity\n",
    "\n",
    "def calculate_perplexity_in_dataset_in_batches(dataset, model, tokenizer, device = DEVICE, batch_size = 8):\n",
    "    model.eval().to(device)\n",
    "    perplexities = []\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        instructions = batch['inputs']\n",
    "        outputs = batch['targets']\n",
    "\n",
    "        for instruction, output in zip(instructions, outputs):\n",
    "            perplexity = calculate_perplexity(instruction, output, model, tokenizer, device)\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "    # 5. Calculate mean perplexity\n",
    "    mean_perplexity = np.mean(perplexities)\n",
    "    std_perplexity = np.std(perplexities)\n",
    "\n",
    "    return mean_perplexity, std_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ['output/aya_dataset_400000_clustered_c20s2_i10_b16a1_l512_r8a16_20240922070346', 'output/aya_dataset_400000_clustered_c20s2_i10_b16a1_l512_r8a16_20240922062739']\n",
    "EVAL_ROUNDS = [1, 50, 100, 200]\n",
    "sim_round = 50\n",
    "\n",
    "experiments = {'0': {'path': PATH[0],\n",
    "                     'type': 'fedavg',\n",
    "                     'rounds': EVAL_ROUNDS,\n",
    "                     'results': []},\n",
    "                     \n",
    "                '1': {'path': PATH[1], \n",
    "                    'type': 'clustered',\n",
    "                      'rounds': EVAL_ROUNDS,\n",
    "                      'results': []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 202362/202362 [00:04<00:00, 49309.49 examples/s]\n",
      "Filter: 100%|██████████| 3670/3670 [00:00<00:00, 41796.81 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mround\u001b[39m \u001b[38;5;129;01min\u001b[39;00m experiments[e][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrounds\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mround\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sim_round:\n\u001b[0;32m---> 15\u001b[0m         model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mstate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         model, tokenizer \u001b[38;5;241m=\u001b[39m state_model(path, \u001b[38;5;28mround\u001b[39m, cluster \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mstate_model\u001b[0;34m(path, round, cluster)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     path \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cluster_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_checkpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnf4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                                    \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_process_index\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, path)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4071\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[1;32m   4069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4070\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 4071\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   4075\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   4076\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   4077\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   4078\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:556\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    554\u001b[0m         extra_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m    555\u001b[0m     weights_only_kwarg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: weights_only}\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mweights_only_kwarg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/torch/serialization.py:1462\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/torch/serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:512\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     ):\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         )\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    514\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/torch/serialization.py:1928\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1928\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/FT_LLM_FL/.venv/lib/python3.10/site-packages/torch/serialization.py:1884\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1882\u001b[0m     storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(nbytes, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m overall_storage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1884\u001b[0m     storage_offset \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_record_offset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1885\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset : storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "languages = ['English', 'Swedish', 'German', 'Portuguese', 'Spanish']\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    print(f'Language: {lang}')\n",
    "    dataset = load_eval_data(DATASET_NAME, EVALSET_LEN, [lang])\n",
    "\n",
    "    for e in experiments:\n",
    "        path = experiments[e]['path']\n",
    "        \n",
    "        if experiments[e]['type'] == 'fedavg':\n",
    "            for round in experiments[e]['rounds']:\n",
    "\n",
    "                if round <= sim_round:\n",
    "                    model, tokenizer = state_model(path, round, cluster = -1)\n",
    "                else:\n",
    "                    model, tokenizer = state_model(path, round, cluster = 0)\n",
    "\n",
    "                mean_perplexity, std_perplexity = calculate_perplexity_in_dataset_in_batches(dataset, model, tokenizer)\n",
    "                exp_type = experiments[e]['type']\n",
    "                print(f'Round: {round}, Path: {path}, Cluster: 0, Type: {exp_type} , Mean Perplexity: {mean_perplexity}, Std Perplexity: {std_perplexity}')\n",
    "\n",
    "                experiments[e]['results'].append({'lang': lang, 'round': round, 'cluster': 0, 'mean_perplexity': mean_perplexity, 'std_perplexity': std_perplexity})\n",
    "        \n",
    "        if experiments[e]['type'] == 'clustered':\n",
    "\n",
    "                for round in experiments[e]['rounds']:\n",
    "                    if round <= sim_round:\n",
    "                        model, tokenizer = state_model(path, round, cluster = -1)\n",
    "                    for cluster in range(5):\n",
    "                        if round <= sim_round:\n",
    "                            model, tokenizer = state_model(path, round, cluster = -1)\n",
    "                        else:\n",
    "                            model, tokenizer = state_model(path, round, cluster = cluster)\n",
    "                        \n",
    "                        if round <= sim_round and cluster > 0: #calculate to only one cluster is already enough (since they are the same)\n",
    "                            continue\n",
    "\n",
    "                        mean_perplexity, std_perplexity = calculate_perplexity_in_dataset_in_batches(dataset, model, tokenizer)\n",
    "                        exp_type = experiments[e]['type']\n",
    "                        print(f'Round: {round}, Path: {path}, Cluster: {cluster}, Type: {exp_type} , Mean Perplexity: {mean_perplexity}, Std Perplexity: {std_perplexity}')\n",
    "\n",
    "                        experiments[e]['results'].append({'lang': lang, 'round': round, 'cluster': cluster, 'mean_perplexity': mean_perplexity, 'std_perplexity': std_perplexity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save experiments results\n",
    "with open('loss_results.json', 'w') as f:\n",
    "    json.dump(experiments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = json.load(open('loss_results.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>round</th>\n",
       "      <th>cluster</th>\n",
       "      <th>mean_perplexity</th>\n",
       "      <th>std_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.198952</td>\n",
       "      <td>148.875317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>54.221013</td>\n",
       "      <td>154.054723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>61.609743</td>\n",
       "      <td>208.040364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>61.855342</td>\n",
       "      <td>202.305250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>285.801540</td>\n",
       "      <td>508.554450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>106.173652</td>\n",
       "      <td>148.762695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>123.683618</td>\n",
       "      <td>182.566626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>140.607875</td>\n",
       "      <td>215.762560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>German</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92.207688</td>\n",
       "      <td>164.752815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>German</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>179.282025</td>\n",
       "      <td>676.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>German</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>200.116887</td>\n",
       "      <td>750.892291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>German</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>220.917039</td>\n",
       "      <td>857.399361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>255.975254</td>\n",
       "      <td>569.694394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>200.978282</td>\n",
       "      <td>446.063594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>222.190007</td>\n",
       "      <td>468.504329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>234.140709</td>\n",
       "      <td>484.136024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.665808</td>\n",
       "      <td>104.805210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>56.639316</td>\n",
       "      <td>82.712836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>64.190427</td>\n",
       "      <td>93.899563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>68.151365</td>\n",
       "      <td>96.215764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lang  round  cluster  mean_perplexity  std_perplexity\n",
       "0      English      1        0        58.198952      148.875317\n",
       "1      English     50        0        54.221013      154.054723\n",
       "2      English    100        0        61.609743      208.040364\n",
       "3      English    200        0        61.855342      202.305250\n",
       "4      Swedish      1        0       285.801540      508.554450\n",
       "5      Swedish     50        0       106.173652      148.762695\n",
       "6      Swedish    100        0       123.683618      182.566626\n",
       "7      Swedish    200        0       140.607875      215.762560\n",
       "8       German      1        0        92.207688      164.752815\n",
       "9       German     50        0       179.282025      676.978500\n",
       "10      German    100        0       200.116887      750.892291\n",
       "11      German    200        0       220.917039      857.399361\n",
       "12  Portuguese      1        0       255.975254      569.694394\n",
       "13  Portuguese     50        0       200.978282      446.063594\n",
       "14  Portuguese    100        0       222.190007      468.504329\n",
       "15  Portuguese    200        0       234.140709      484.136024\n",
       "16     Spanish      1        0        71.665808      104.805210\n",
       "17     Spanish     50        0        56.639316       82.712836\n",
       "18     Spanish    100        0        64.190427       93.899563\n",
       "19     Spanish    200        0        68.151365       96.215764"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(experiments['0']['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>round</th>\n",
       "      <th>cluster</th>\n",
       "      <th>mean_perplexity</th>\n",
       "      <th>std_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.661850</td>\n",
       "      <td>150.168873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>54.245254</td>\n",
       "      <td>156.982793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>53.552095</td>\n",
       "      <td>147.766082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>54.887487</td>\n",
       "      <td>166.477956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>58.114222</td>\n",
       "      <td>178.885767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>English</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>62.612469</td>\n",
       "      <td>214.585635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>English</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>61.492710</td>\n",
       "      <td>213.434596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>English</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>53.275978</td>\n",
       "      <td>144.172591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>English</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>58.445066</td>\n",
       "      <td>194.135281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>60.158384</td>\n",
       "      <td>195.870439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>English</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>66.683820</td>\n",
       "      <td>224.607381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>English</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>63.271586</td>\n",
       "      <td>217.314488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>286.638246</td>\n",
       "      <td>511.023638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>109.515776</td>\n",
       "      <td>153.707798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>108.213613</td>\n",
       "      <td>153.851547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>106.425413</td>\n",
       "      <td>147.927679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>129.962993</td>\n",
       "      <td>193.040714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>125.486420</td>\n",
       "      <td>180.387998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>125.892932</td>\n",
       "      <td>185.847337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>112.694009</td>\n",
       "      <td>161.298891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>112.156557</td>\n",
       "      <td>158.668510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>136.486058</td>\n",
       "      <td>205.686691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>145.261235</td>\n",
       "      <td>215.462362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Swedish</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>139.456989</td>\n",
       "      <td>211.381562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>German</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>93.077104</td>\n",
       "      <td>167.411530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>German</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>179.078083</td>\n",
       "      <td>690.743802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>German</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>190.371708</td>\n",
       "      <td>771.457285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>German</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>178.429390</td>\n",
       "      <td>687.810625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>German</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>193.020014</td>\n",
       "      <td>717.568665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>German</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>195.573401</td>\n",
       "      <td>754.612293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>German</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>196.649591</td>\n",
       "      <td>728.819992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>German</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>194.300167</td>\n",
       "      <td>779.578647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>German</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>194.258259</td>\n",
       "      <td>761.485806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>German</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>195.835948</td>\n",
       "      <td>711.539213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>German</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>194.332591</td>\n",
       "      <td>713.616776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>German</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>219.933317</td>\n",
       "      <td>815.372963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>254.810184</td>\n",
       "      <td>563.910088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>190.967244</td>\n",
       "      <td>415.707517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>196.811989</td>\n",
       "      <td>437.089961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>191.542587</td>\n",
       "      <td>404.902757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>208.851638</td>\n",
       "      <td>455.624792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>197.988858</td>\n",
       "      <td>416.737569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>206.878249</td>\n",
       "      <td>457.401955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>201.189994</td>\n",
       "      <td>431.145806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>199.687100</td>\n",
       "      <td>421.659445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>210.309888</td>\n",
       "      <td>450.943646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>218.760023</td>\n",
       "      <td>473.279660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>219.372029</td>\n",
       "      <td>495.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72.257722</td>\n",
       "      <td>108.037598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>54.646659</td>\n",
       "      <td>77.245407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>55.975673</td>\n",
       "      <td>77.550574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>55.211799</td>\n",
       "      <td>77.497931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>61.551928</td>\n",
       "      <td>84.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>59.349473</td>\n",
       "      <td>91.383959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>58.071917</td>\n",
       "      <td>82.836930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>59.773084</td>\n",
       "      <td>81.417233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>57.598479</td>\n",
       "      <td>80.323776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>63.551160</td>\n",
       "      <td>86.632020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>66.459092</td>\n",
       "      <td>99.869674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>61.099646</td>\n",
       "      <td>86.352947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lang  round  cluster  mean_perplexity  std_perplexity\n",
       "0      English      1        0        58.661850      150.168873\n",
       "1      English     50        0        54.245254      156.982793\n",
       "2      English    100        0        53.552095      147.766082\n",
       "3      English    100        1        54.887487      166.477956\n",
       "4      English    100        2        58.114222      178.885767\n",
       "5      English    100        3        62.612469      214.585635\n",
       "6      English    100        4        61.492710      213.434596\n",
       "7      English    200        0        53.275978      144.172591\n",
       "8      English    200        1        58.445066      194.135281\n",
       "9      English    200        2        60.158384      195.870439\n",
       "10     English    200        3        66.683820      224.607381\n",
       "11     English    200        4        63.271586      217.314488\n",
       "12     Swedish      1        0       286.638246      511.023638\n",
       "13     Swedish     50        0       109.515776      153.707798\n",
       "14     Swedish    100        0       108.213613      153.851547\n",
       "15     Swedish    100        1       106.425413      147.927679\n",
       "16     Swedish    100        2       129.962993      193.040714\n",
       "17     Swedish    100        3       125.486420      180.387998\n",
       "18     Swedish    100        4       125.892932      185.847337\n",
       "19     Swedish    200        0       112.694009      161.298891\n",
       "20     Swedish    200        1       112.156557      158.668510\n",
       "21     Swedish    200        2       136.486058      205.686691\n",
       "22     Swedish    200        3       145.261235      215.462362\n",
       "23     Swedish    200        4       139.456989      211.381562\n",
       "24      German      1        0        93.077104      167.411530\n",
       "25      German     50        0       179.078083      690.743802\n",
       "26      German    100        0       190.371708      771.457285\n",
       "27      German    100        1       178.429390      687.810625\n",
       "28      German    100        2       193.020014      717.568665\n",
       "29      German    100        3       195.573401      754.612293\n",
       "30      German    100        4       196.649591      728.819992\n",
       "31      German    200        0       194.300167      779.578647\n",
       "32      German    200        1       194.258259      761.485806\n",
       "33      German    200        2       195.835948      711.539213\n",
       "34      German    200        3       194.332591      713.616776\n",
       "35      German    200        4       219.933317      815.372963\n",
       "36  Portuguese      1        0       254.810184      563.910088\n",
       "37  Portuguese     50        0       190.967244      415.707517\n",
       "38  Portuguese    100        0       196.811989      437.089961\n",
       "39  Portuguese    100        1       191.542587      404.902757\n",
       "40  Portuguese    100        2       208.851638      455.624792\n",
       "41  Portuguese    100        3       197.988858      416.737569\n",
       "42  Portuguese    100        4       206.878249      457.401955\n",
       "43  Portuguese    200        0       201.189994      431.145806\n",
       "44  Portuguese    200        1       199.687100      421.659445\n",
       "45  Portuguese    200        2       210.309888      450.943646\n",
       "46  Portuguese    200        3       218.760023      473.279660\n",
       "47  Portuguese    200        4       219.372029      495.169000\n",
       "48     Spanish      1        0        72.257722      108.037598\n",
       "49     Spanish     50        0        54.646659       77.245407\n",
       "50     Spanish    100        0        55.975673       77.550574\n",
       "51     Spanish    100        1        55.211799       77.497931\n",
       "52     Spanish    100        2        61.551928       84.606500\n",
       "53     Spanish    100        3        59.349473       91.383959\n",
       "54     Spanish    100        4        58.071917       82.836930\n",
       "55     Spanish    200        0        59.773084       81.417233\n",
       "56     Spanish    200        1        57.598479       80.323776\n",
       "57     Spanish    200        2        63.551160       86.632020\n",
       "58     Spanish    200        3        66.459092       99.869674\n",
       "59     Spanish    200        4        61.099646       86.352947"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(experiments['1']['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
